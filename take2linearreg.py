# -*- coding: utf-8 -*-
"""take2linearReg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uZmJzPKFBON3Uz37GjMzVOVeeQZvyaWe

#imports
"""

import numpy as np
import pandas as pd
from geopy.geocoders import Nominatim
import matplotlib.pyplot as plt
import math

data = pd.read_csv('/content/raw_house_data.csv')

"""#My preprocessing library

##How to use:

Standard usage for my library:

**To instantiate the class**

- preprocessor = DataPreprocessor()

**Using the convert_to_int method**

 - preprocessor.convert_to_int(**dataframe**, '**column**')

 - By default this should also run the **check_non_numeric** as a sub method.

**Using the resolve_categorical method**

- preprocessor.resolve_categorical(**dataframe**, '**existing_column_name**')

- existing_column_name = dataframe['existing_column_name'].unique()

- data['new_column_name'] = data['existing_column_name'].map({feature: i for i, feature in enumerate(kitchen_features)})

**Using the round floats method**

- preprocessor.round_floats(**dataframe**, '**column**')

**Using scale_features**
- data_scaled = preprocessor.scale_features(**dataframe**, **list_of_columns**)

**Using remove_outliers**

- data_no_outliers = preprocessor.remove_outliers(**data_scaled_dataframe**, **list_of_columns**k, threshold=3)

**Using display_correlation_matrix method**

- data_processor.display_correlation_matrix(**list_of_columns**)
"""

#import pandas as pd
#import numpy as np
#import math
#import matplotlib.pyplot as plt

class DataPreprocessor:
    def __init__(self, data):
        self.data = data

    def remove_negative_sign(self, X):
        """
        Removes negative sign from numerical data.
        """
        X_copy = X.copy()
        if isinstance(X_copy, pd.Series):
            X_copy = X_copy.to_numpy().reshape(-1, 1)
        for i in range(X_copy.shape[1]):
            if np.issubdtype(X_copy[:, i].dtype, np.number):
                X_copy[:, i] = np.abs(X_copy[:, i])
        return X_copy

    def scale_features(self, data, columns, method='standardization'):
        """
        Scales or normalizes the features of a DataFrame.

        Parameters:
        - data: Pandas DataFrame containing the data.
        - columns: list of columns to scale or normalize.
        - method: scaling method to use. Can be 'standardization' or 'min-max'.

        Returns:
        - Pandas DataFrame with the scaled or normalized features.
        """
        # Make a copy of the data to avoid modifying the original DataFrame
        data_scaled = data.copy()

        # Scale or normalize the selected columns
        if method == 'standardization':
            for col in columns:
                data_scaled[col] = (data_scaled[col] - data_scaled[col].mean()) / data_scaled[col].std()

        elif method == 'min-max':
            for col in columns:
                data_scaled[col] = (data_scaled[col] - data_scaled[col].min()) / (data_scaled[col].max() - data_scaled[col].min())

        return data_scaled

    def remove_outliers(self, data, columns, threshold=2):
        """
        Removes outliers from a DataFrame using the z-score method.

        Parameters:
        - data: Pandas DataFrame containing the data.
        - columns: list of columns to check for outliers.
        - threshold: number of standard deviations from the mean to consider an outlier.

        Returns:
        - Pandas DataFrame without the outliers.
        """
        # Make a copy of the data to avoid modifying the original DataFrame
        data_clean = data.copy()

        # Iterate over the columns and remove outliers using the z-score method
        for col in columns:
            z_scores = np.abs((data_clean[col] - data_clean[col].mean()) / data_clean[col].std())
            data_clean = data_clean[z_scores <= threshold * data_clean[col].std()]

        return data_clean

    @staticmethod
    def check_non_numeric(data, column):
        # Select all rows in the column that cannot be converted to a numeric type
        non_numeric = data[pd.to_numeric(data[column], errors='coerce').isna()]

        # Check if there are any non-numeric values in the selected rows
        if len(non_numeric) > 0:
            # If there are non-numeric values, print them out
            print(f"Found non-numeric values in column '{column}':")
            print(non_numeric)
        else:
            # If there are no non-numeric values, print a message indicating so
            print(f"No non-numeric values found in column '{column}'")
            
    @staticmethod
    def resolve_categorical(data, column):
        # Check if the column has any missing values
        if data[column].isna().sum() > 0:
            # If there are missing values, drop them
            data.dropna(subset=[column], inplace=True)
        
        # Replace "None" with 0
        data[column] = data[column].replace("None", 0)
        
        # Count the categorical values
        cat_counts = data[column].value_counts().to_dict()

        # Replace the original values with their counts
        data[column] = data[column].map(cat_counts)

        # Verify the result
        print(data[column].unique())


    def convert_to_int(self, data, column):
        # Check for non-numeric values
        DataPreprocessor.check_non_numeric(data, column)

        # Replace "None" with NaN
        data[column] = data[column].replace("None", 0)

        # Convert the column to string
        data[column] = data[column].astype(str)

        # Remove commas from numbers
        data[column] = data[column].str.replace(",", "")

        # Convert the column to float
        data[column] = data[column].astype(float)

        # Replace NaN with 0
        data[column] = data[column].fillna(0)

        # Round down to the nearest integer
        data[column] = data[column].apply(lambda x: math.floor(x))

        # Convert to int
        data[column] = data[column].astype(int)

        # Verify the result
        print(data[column].unique())
        
    def convert_to_float(self, data, column):
        # Check for non-numeric values
        DataPreprocessor.check_non_numeric(data, column)

        # Replace "None" with NaN
        data[column] = data[column].replace("None", 0)


        # Convert the column to string
        data[column] = data[column].astype(str)

        # Remove commas from numbers
        data[column] = data[column].str.replace(",", "")

        # Check for non-numeric values
        DataPreprocessor.check_non_numeric(data, column)
        
        # Replace "None" with NaN
        data[column] = data[column].replace("None", np.nan)

        # Convert the column to float
        data[column] = data[column].astype(float)

        # Replace NaN with 0
        data[column] = data[column].fillna(0)

        # Verify the result
        print(data[column].unique())

    def round_floats(self, data, column):
        # Check if the column has any missing values
        if data[column].isna().sum() > 0:
            # If there are missing values, drop them
            data.dropna(subset=[column], inplace=True)
        
        # Round floats to 2 decimal points or add 0 if only one
        data[column] = data[column].apply(lambda x: '{:.2f}'.format(x) if isinstance(x, float) and x.is_integer() == False else '{:.2f}0'.format(x) if isinstance(x, float) and x.is_integer() else x)
        
        # Verify the result
        print(data[column].unique())

    def display_correlation_matrix(self, columns_to_check):
      
        # Calculate the correlation matrix
        correlation_matrix = self.data[columns_to_check].corr()

        fig, ax = plt.subplots(figsize=(10, 8))
        im = ax.imshow(correlation_matrix, cmap='coolwarm')

        # Display the colorbar
        cbar = ax.figure.colorbar(im, ax=ax)

        # Set the tick labels and axis labels
        ax.set_xticks(np.arange(len(columns_to_check)))
        ax.set_yticks(np.arange(len(columns_to_check)))
        ax.set_xticklabels(columns_to_check, fontsize=12)
        ax.set_yticklabels(columns_to_check, fontsize=12)
        ax.set_xlabel('Features', fontsize=14)
        ax.set_ylabel('Features', fontsize=14)

        # Rotate the tick labels and set their alignment
        plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
                 rotation_mode="anchor")

        # Display the correlation values in the heatmap
        for i in range(len(columns_to_check)):
            for j in range(len(columns_to_check)):
                text = ax.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}', ha="center", va="center", color="w", fontsize=10)

        # Set the title of the plot
        ax.set_title("Correlation Matrix", fontsize=16)

        # Show the plot
        plt.show()

data.dtypes

# Verify that there are no more missing values
missing_values = data.isnull().sum()
print(missing_values)

# Fill in missing values in fireplaces with the mode
mode_fireplaces = data['fireplaces'].mode()[0]
data['fireplaces'].fillna(mode_fireplaces, inplace=True)

# Fill in missing values in lot_acres with the median
median_lot_acres = data['lot_acres'].median()
data['lot_acres'].fillna(median_lot_acres, inplace=True)

# Print the number of missing values for each column
print(missing_values)

# Check for missing values
missing_values = data.isnull().sum()

"""#Preprocessing (refactored)"""

preprocessor = DataPreprocessor(data)

data['sold_price'] = preprocessor.remove_negative_sign(data['sold_price'])

preprocessor.convert_to_float(data, 'sold_price')

preprocessor.convert_to_int(data, 'year_built')

preprocessor.convert_to_int(data, 'bathrooms')

preprocessor.convert_to_int(data, 'garage')

preprocessor.convert_to_int(data, 'sqrt_ft')

# check the unique values present in the column and investigate if there are any non-numeric values
data['kitchen_features'].unique()

preprocessor.convert_to_float(data, 'taxes')

# Example usage on kitchen_features column
preprocessor.resolve_categorical(data, 'kitchen_features')
kitchen_features = data['kitchen_features'].unique()
data['kitchen_features_values'] = data['kitchen_features'].map({feature: i for i, feature in enumerate(kitchen_features)})

# Example usage on kitchen_features column
preprocessor.resolve_categorical(data, 'floor_covering')
floor_covering = data['floor_covering'].unique()
data['floor_covering_values'] = data['floor_covering'].map({feature: i for i, feature in enumerate(floor_covering)})

# check the unique values present in the column and investigate if there are any non-numeric values
data['floor_covering'].unique()

preprocessor.convert_to_int(data, 'HOA')

data.dtypes

# Check for missing values
print(data.isnull().sum())

"""#Regression models

##The regression classes and methods
"""

# Create a Linear Regression class
class LinearRegression:
    def __init__(self, learning_rate=0.01, n_iterations=1000):
        self.learning_rate = learning_rate
        self.n_iterations = n_iterations

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        self.bias = 0

        for i in range(self.n_iterations):
            y_pred = np.dot(X, self.weights) + self.bias
            dw = (1 / n_samples) * np.dot(X.T, (y_pred - y))
            db = (1 / n_samples) * np.sum(y_pred - y)
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db

    def predict(self, X):
        return np.dot(X, self.weights) + self.bias

# Create a KNN Regressor class
class KNNRegressor():
    def fit(self, X, y):
        self.X = X
        self.y = y

    def predict(self, X, K, epsilon=1e-3):
        N = len(X)
        y_hat = np.zeros(N)

        for i in range(N):
            dist2 = np.sum((self.X - X[i])**2, axis=1)
            idxt = np.argsort(dist2)[:K]
            gamma_K = np.exp(-dist2[idxt]) / np.exp(-dist2[idxt]).sum()
            y_hat[i] = gamma_K.dot(self.y[idxt])

        return y_hat

def gradient_descent(m_now, b_now, data, learning_rate):
    m_gradient = 0
    b_gradient = 0

    n = len(data)

    for i in range(n):
        x = data.iloc[i].bedrooms
        y = data.iloc[i].sold_price - data.iloc[i].price_after_renovations
        
        m_gradient += -(2/n) * x * (y - (m_now * x + b_now))
        b_gradient += -(2/n) * (y - (m_now * x + b_now))

    m = m_now - m_gradient * learning_rate
    b = b_now - b_gradient * learning_rate
    return m, b

def gradient_descent(m_now, b_now, data, learning_rate):
    m_gradient = 0
    b_gradient = 0

    n = len(data)

    for i in range(n):
        x = data.iloc[i].bathrooms
        y = data.iloc[i].sold_price - data.iloc[i].price_after_renovations
        
        m_gradient += -(2/n) * x * (y - (m_now * x + b_now))
        b_gradient += -(2/n) * (y - (m_now * x + b_now))

    m = m_now - m_gradient * learning_rate
    b = b_now - b_gradient * learning_rate
    return m, b

data['bedrooms_new'] = data['bedrooms']

data['bathrooms_new'] = data['bathrooms']

# Select the columns for linear regression
cols_lr = ['fireplaces', 'bathrooms', 'bedrooms', 'garage', 'floor_covering_values', 'bedrooms_new','bathrooms_new']
# Extract the features and target variable for linear regression
X_lr = data[cols_lr].values
y_lr = data['sold_price'].values

# Select the columns for KNN regression
cols_knn = cols_lr = ['fireplaces', 'bathrooms', 'bedrooms', 'garage', 'floor_covering_values', 'bedrooms_new','bathrooms_new']
X_knn = data[cols_knn].values
y_knn = data['sold_price'].values

# Scale the features using min-max scaling
X_lr = (X_lr - X_lr.min(axis=0)) / (X_lr.max(axis=0) - X_lr.min(axis=0))

"""The LinearRegression class has two methods, fit and predict. The fit method takes in the input matrix X and the target vector y and updates the weights and bias using gradient descent. The predict method takes in a new input matrix X and returns the predictions for each sample.


To use the LinearRegression class, you can create an instance of the class and call the fit method to train the model on some data, and then call the predict method to make predictions on new data:
"""

# Train the linear regression model
lr = LinearRegression()
lr.fit(X_lr, y_lr)

# Train the KNN regressor model
knn = KNNRegressor()
knn.fit(X_knn, y_knn)

# Scale the features using min-max scaling
X_knn = (X_knn - X_knn.min(axis=0)) / (X_knn.max(axis=0) - X_knn.min(axis=0))

"""##Get coordinates (Address entry)

###Addresses

2001 E 6th St, Tucson, AZ 85719

3042 Placita, Tucson, AZ 85716
"""

address1="2001 E 6th St, Tucson, AZ 85719"

address3="3042 Placita, Tucson, AZ 85716"

# Get the user input for the property address
address = input("Enter the address of the property: ")

geolocator = Nominatim(user_agent="mAIstros")
location = geolocator.geocode(address)

if location is not None:
    lat = location.latitude
    lon = location.longitude
    print(f"Latitude: {lat}, Longitude: {lon}")
else:
    print(f"Could not find location for address: {address}")

#Create a new data point with user input and geocoded location
new_data = np.array([[0, 1, 1, 0, 1, 0], [0, 0, 1, 0, 0, 1]])

# Select the same columns as X_lr
new_X = data[cols_lr].values

new_Xknn = data[cols_knn].values

# Scale the new data point
new_X_scaled = (new_X - X_lr.min(axis=0)) / (X_lr.max(axis=0) - X_lr.min(axis=0))

# Predict the sale price using Linear Regression
sale_price_lr = lr.predict(new_X_scaled)

# Remove the extra column from new_data
new_data = new_data[:, :-1]

K = 3
sale_price_knn = knn.predict(new_Xknn, K)

"""** Bedroom and bathroom prediction (refactoring) **

##retrieving the data
"""

# Create a new data point with the user input
bedrooms = int(input("Enter the number of bedrooms: "))
bathrooms = int(input("Enter the number of bathrooms: "))

# Create a new DataFrame with the user input
new_data = pd.DataFrame({'fireplaces': [0], 'bathrooms': [bathrooms], 'bedrooms': [bedrooms], 'garage': [0], 
                         'bedrooms_new': [bedrooms], 'bathrooms_new': [bathrooms] , 'floor_covering_values': [0]})

# Select the same columns as X_knn from new_data
new_data_knn = new_data[cols_knn].values
price_after_renovations_knn = knn.predict(new_data_knn.reshape(1, -1), K)


# Select the same columns as X_lr
new_X = data[cols_lr].values

# Scale the new data point
new_X_scaled = (new_X - X_lr.min(axis=0)) / (X_lr.max(axis=0) - X_lr.min(axis=0))

# Make new_data the same size as X_lr
new_data_full = new_data[cols_lr].values
price_after_renovations_lr = lr.predict(new_data_full.reshape(1, -1))

# Predict the price after renovations using Linear Regression
price_after_renovations_lr = lr.predict(new_data_full)

# Predict the price after renovations using KNN Regression with K = 3
K = 3
price_after_renovations_knn = knn.predict(new_data_knn.reshape(1, -1), K)

# Print the predicted price after renovations for the new data point
print(f"The estimated price after renovations for a {bedrooms}-bedroom and {bathrooms}-bathroom house is:")
print(f"- ${float(price_after_renovations_lr[0]):.2f} using Linear Regression")
print(f"- ${float(price_after_renovations_knn[0]):.2f} using KNN Regression")

# Calculate the total predicted price after renovations
total_price_after_renovations = float(price_after_renovations_lr[0]) + float(price_after_renovations_knn[0])

print(f"The estimated price after renovations for a {bedrooms}-bedroom and {bathrooms}-bathroom house is ${float(price_after_renovations_lr[0]):.2f} (Linear Regression) and ${float(price_after_renovations_knn[0]):.2f} (KNN Regression), with a total of ${total_price_after_renovations:.2f} after renovations.")

"""predict the new renovated house value"""

#import pandas as pd
#from sklearn.neighbors import KNeighborsRegressor
#from sklearn.linear_model import LinearRegression

# Select a random row from the dataset with the same number of bedrooms and bathrooms as the new data point
bedrooms = 4
bathrooms = 4
sample_data = data[(data['bedrooms'] == bedrooms) & (data['bathrooms'] == bathrooms)].sample(n=1)

# Extract the target value for the sample data
y_true = sample_data['sold_price'].values[0]

# Extract the features for the sample data
X_sample = sample_data[cols_lr].values

# Scale the features for the sample data
X_sample_scaled = (X_sample - X_lr.min(axis=0)) / (X_lr.max(axis=0) - X_lr.min(axis=0))

#Predict the price using Linear Regression for the sample data
price_lr = lr.predict(X_sample_scaled.reshape(1, -1))

#Predict the price using KNN Regression with K=3 for the sample data
price_knn = knn.predict(X_sample.reshape(1, -1), K=3)

#Calculate the average price for Linear Regression and KNN Regression
price_avg = (price_lr[0] + price_knn[0]) / 2

#Calculate the accuracy of the prediction for Linear Regression
acc_lr = (1 - abs(price_lr[0] - y_true) / y_true) * 100

#Calculate the accuracy of the prediction for KNN Regression
acc_knn = (1 - abs(price_knn[0] - y_true) / y_true) * 100

#Calculate the accuracy of the prediction for the average of Linear Regression and KNN Regression
acc_avg = (1 - abs(price_avg - y_true) / y_true) * 100

#Print the accuracy of the prediction for Linear Regression, KNN Regression, and the average of the two
print(f"Linear Regression prediction accuracy: {acc_lr:.2f}%")
print(f"KNN Regression prediction accuracy: {acc_knn:.2f}%")
print(f"Average prediction accuracy: {acc_avg:.2f}%")

# Calculate the predicted price for the sample data point using both linear regression and KNN regression
sample_data_knn = sample_data[cols_knn].values
y_pred_knn = knn.predict(sample_data_knn.reshape(1, -1), K=3)

# Calculate the difference between the predicted and actual prices
knn_diff = y_pred_knn[0] - y_true

# Plot the difference in predicted price versus the actual price
plt.scatter(y_true, knn_diff, label='KNN Regression')
plt.axhline(y=0, color='black', linestyle='--')
plt.xlabel('Actual Price')
plt.ylabel('Difference in Predicted Price')
plt.legend()
plt.show()

"""#bathroom entry and prediction (deprecated)"""

# Scale the new data point
#new_X_scaled = (new_X - X_lr.min(axis=0)) / (X_lr.max(axis=0) - X_lr.min(axis=0))

# Predict the sale price using Linear Regression
#sold_price = lr.predict(new_X_scaled)

# Predict the sale price using KNN Regression
#sale_price_knn = knn.predict(new_X_scaled, K=5)

'''
# Create a new data point with the user input
bathrooms = int(input("Enter the number of bathrooms: "))

# Select the same columns as X_lr
new_X = new_data[cols_lr].values

# Scale the new data point
new_X_scaled = (new_X - X_lr.min(axis=0)) / (X_lr.max(axis=0) - X_lr.min(axis=0))

# Make a prediction
price_after_renovations = lr.predict(new_X_scaled.reshape(1,-1))[0]

print(f"The estimated price after renovations for a {bathrooms}-bedroom house is ${float(price_after_renovations):.2f}")
'''

"""#bedroom entry and prediction (deprecated)"""

#Print the predicted sale price using both models
print(f"Predicted sale price using Linear Regression: {sale_price_lr}")
print(f"Predicted sale price using KNN Regression: {sale_price_knn}")

data['price_after_renovations'] = data['sold_price']

'''
# Create a new data point with the user input
bedrooms_new = int(input("Enter the number of bedrooms: "))

# Select the same columns as X_lr
new_X = new_data[cols_lr].values

# Scale the new data point
new_X_scaled = (new_X - X_lr.min(axis=0)) / (X_lr.max(axis=0) - X_lr.min(axis=0))

# Make a prediction
price_after_renovations = lr.predict(new_X_scaled.reshape(1,-1))[0]

print(f"The estimated price after renovations for {bedrooms}-bedrooms is ${float(price_after_renovations):.2f}")
'''

#Plot the relationship between the number of bedrooms and the sold price

plt.scatter(data['bedrooms'], data['sold_price'], alpha=0.5)
plt.xlabel('Number of Bedrooms')
plt.ylabel('Sold Price ($)')
plt.title('Relationship between Number of Bedrooms and Sold Price')
plt.show()

data['price_diff'] = data['sold_price'] - data['price_after_renovations']

plt.scatter(data['bedrooms'], data['price_diff'], alpha=0.5)
plt.xlabel('Number of Bedrooms')
plt.ylabel('Price Difference ($)')
plt.title('Difference between Original Sales Price and Predicted Price after Renovations')
plt.show()