# -*- coding: utf-8 -*-
"""HouseLinReg-Price-Renovations.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11UbtXijUMJ29XMZ6QsbWPY1gM0wkunRw
"""

import pandas as pd
import numpy as np
import math
import matplotlib.pyplot as plt

data = pd.read_csv('/content/raw_house_data.csv')

"""#Check data is loaded"""

print(data.head())

print(data.shape)

"""#Check missing values"""

# Check for missing values
missing_values = data.isnull().sum()

# Print the number of missing values for each column
print(missing_values)

"""#Fill missing values"""

# Fill in missing values in lot_acres with the median
median_lot_acres = data['lot_acres'].median()
data['lot_acres'].fillna(median_lot_acres, inplace=True)

# Fill in missing values in fireplaces with the mode
mode_fireplaces = data['fireplaces'].mode()[0]
data['fireplaces'].fillna(mode_fireplaces, inplace=True)

"""#Verify missing values are filled"""

# Verify that there are no more missing values
missing_values = data.isnull().sum()
print(missing_values)

"""###Double verification"""

print(data.isnull().sum())

"""#Check Data types"""

data.dtypes

"""#Created a class to treat data

##How to use:

Standard usage for my library:

**To instantiate the class**

- preprocessor = DataPreprocessor()

**Using the convert_to_int method**

 - preprocessor.convert_to_int(**dataframe**, '**column**')

 - By default this should also run the **check_non_numeric** as a sub method.

**Using the resolve_categorical method**

- preprocessor.resolve_categorical(**dataframe**, '**existing_column_name**')

- existing_column_name = dataframe['existing_column_name'].unique()

- data['new_column_name'] = data['existing_column_name'].map({feature: i for i, feature in enumerate(kitchen_features)})

**Using the round floats method**

- preprocessor.round_floats(**dataframe**, '**column**')

**Using scale_features**
- data_scaled = preprocessor.scale_features(**dataframe**, **list_of_columns**)

**Using remove_outliers**

- data_no_outliers = preprocessor.remove_outliers(**data_scaled_dataframe**, **list_of_columns**k, threshold=3)

**Using display_correlation_matrix method**

- data_processor.display_correlation_matrix(**list_of_columns**)
"""

#import pandas as pd
#import numpy as np
#import math
#import matplotlib.pyplot as plt

class DataPreprocessor:
    def __init__(self, data):
        self.data = data

    def scale_features(self, data, columns, method='standardization'):
        """
        Scales or normalizes the features of a DataFrame.

        Parameters:
        - data: Pandas DataFrame containing the data.
        - columns: list of columns to scale or normalize.
        - method: scaling method to use. Can be 'standardization' or 'min-max'.

        Returns:
        - Pandas DataFrame with the scaled or normalized features.
        """
        # Make a copy of the data to avoid modifying the original DataFrame
        data_scaled = data.copy()

        # Scale or normalize the selected columns
        if method == 'standardization':
            for col in columns:
                data_scaled[col] = (data_scaled[col] - data_scaled[col].mean()) / data_scaled[col].std()

        elif method == 'min-max':
            for col in columns:
                data_scaled[col] = (data_scaled[col] - data_scaled[col].min()) / (data_scaled[col].max() - data_scaled[col].min())

        return data_scaled

    def remove_outliers(self, data, columns, threshold=2):
        """
        Removes outliers from a DataFrame using the z-score method.

        Parameters:
        - data: Pandas DataFrame containing the data.
        - columns: list of columns to check for outliers.
        - threshold: number of standard deviations from the mean to consider an outlier.

        Returns:
        - Pandas DataFrame without the outliers.
        """
        # Make a copy of the data to avoid modifying the original DataFrame
        data_clean = data.copy()

        # Iterate over the columns and remove outliers using the z-score method
        for col in columns:
            z_scores = np.abs((data_clean[col] - data_clean[col].mean()) / data_clean[col].std())
            data_clean = data_clean[z_scores <= threshold * data_clean[col].std()]

        return data_clean

    @staticmethod
    def check_non_numeric(data, column):
        # Select all rows in the column that cannot be converted to a numeric type
        non_numeric = data[pd.to_numeric(data[column], errors='coerce').isna()]

        # Check if there are any non-numeric values in the selected rows
        if len(non_numeric) > 0:
            # If there are non-numeric values, print them out
            print(f"Found non-numeric values in column '{column}':")
            print(non_numeric)
        else:
            # If there are no non-numeric values, print a message indicating so
            print(f"No non-numeric values found in column '{column}'")
            
    @staticmethod
    def resolve_categorical(data, column):
        # Check if the column has any missing values
        if data[column].isna().sum() > 0:
            # If there are missing values, drop them
            data.dropna(subset=[column], inplace=True)
        
        # Replace "None" with 0
        data[column] = data[column].replace("None", 0)
        
        # Count the categorical values
        cat_counts = data[column].value_counts().to_dict()

        # Replace the original values with their counts
        data[column] = data[column].map(cat_counts)

        # Verify the result
        print(data[column].unique())


    def convert_to_int(self, data, column):
        # Check for non-numeric values
        DataPreprocessor.check_non_numeric(data, column)

        # Replace "None" with NaN
        data[column] = data[column].replace("None", 0)

        # Convert the column to string
        data[column] = data[column].astype(str)

        # Remove commas from numbers
        data[column] = data[column].str.replace(",", "")

        # Convert the column to float
        data[column] = data[column].astype(float)

        # Replace NaN with 0
        data[column] = data[column].fillna(0)

        # Round down to the nearest integer
        data[column] = data[column].apply(lambda x: math.floor(x))

        # Convert to int
        data[column] = data[column].astype(int)

        # Verify the result
        print(data[column].unique())
        
    def convert_to_float(self, data, column):
        # Check for non-numeric values
        DataPreprocessor.check_non_numeric(data, column)

        # Replace "None" with NaN
        data[column] = data[column].replace("None", 0)


        # Convert the column to string
        data[column] = data[column].astype(str)

        # Remove commas from numbers
        data[column] = data[column].str.replace(",", "")

        # Check for non-numeric values
        DataPreprocessor.check_non_numeric(data, column)
        
        # Replace "None" with NaN
        data[column] = data[column].replace("None", np.nan)

        # Convert the column to float
        data[column] = data[column].astype(float)

        # Replace NaN with 0
        data[column] = data[column].fillna(0)

        # Verify the result
        print(data[column].unique())

    def round_floats(self, data, column):
        # Check if the column has any missing values
        if data[column].isna().sum() > 0:
            # If there are missing values, drop them
            data.dropna(subset=[column], inplace=True)
        
        # Round floats to 2 decimal points or add 0 if only one
        data[column] = data[column].apply(lambda x: '{:.2f}'.format(x) if isinstance(x, float) and x.is_integer() == False else '{:.2f}0'.format(x) if isinstance(x, float) and x.is_integer() else x)
        
        # Verify the result
        print(data[column].unique())

    def display_correlation_matrix(self, columns_to_check):
      
        # Calculate the correlation matrix
        correlation_matrix = self.data[columns_to_check].corr()

        fig, ax = plt.subplots(figsize=(10, 8))
        im = ax.imshow(correlation_matrix, cmap='coolwarm')

        # Display the colorbar
        cbar = ax.figure.colorbar(im, ax=ax)

        # Set the tick labels and axis labels
        ax.set_xticks(np.arange(len(columns_to_check)))
        ax.set_yticks(np.arange(len(columns_to_check)))
        ax.set_xticklabels(columns_to_check, fontsize=12)
        ax.set_yticklabels(columns_to_check, fontsize=12)
        ax.set_xlabel('Features', fontsize=14)
        ax.set_ylabel('Features', fontsize=14)

        # Rotate the tick labels and set their alignment
        plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
                 rotation_mode="anchor")

        # Display the correlation values in the heatmap
        for i in range(len(columns_to_check)):
            for j in range(len(columns_to_check)):
                text = ax.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}', ha="center", va="center", color="w", fontsize=10)

        # Set the title of the plot
        ax.set_title("Correlation Matrix", fontsize=16)

        # Show the plot
        plt.show()

"""#Instantiate Preprocessor"""

preprocessor = DataPreprocessor()

"""#Treating Data with preprocessor class

##Price
"""

preprocessor.round_floats(data, 'sold_price')

preprocessor.convert_to_float(data, 'sold_price')

"""##Year built"""

preprocessor.convert_to_int(data, 'year_built')

"""##Taxes"""

preprocessor.round_floats(data, 'taxes')

preprocessor.convert_to_float(data, 'taxes')

"""##Bathroom"""

preprocessor.convert_to_int(data, 'bathrooms')

"""##Garage"""

preprocessor.convert_to_int(data, 'garage')

"""##Sqrt_ft"""

preprocessor.convert_to_int(data, 'sqrt_ft')

"""##kitchen_features"""

# check the unique values present in the column and investigate if there are any non-numeric values
data['kitchen_features'].unique()

"""In the code below, we first assign the unique values of the kitchen_features column to the kitchen_features variable. Then we create a dictionary with the feature names as keys and their corresponding index as values. Finally, we use the map() method to replace each feature name with its index."""

# Example usage on kitchen_features column
preprocessor.resolve_categorical(data, 'kitchen_features')
kitchen_features = data['kitchen_features'].unique()
data['kitchen_features_values'] = data['kitchen_features'].map({feature: i for i, feature in enumerate(kitchen_features)})

"""##floor_covering"""

# check the unique values present in the column and investigate if there are any non-numeric values
data['floor_covering'].unique()

# Example usage on kitchen_features column
preprocessor.resolve_categorical(data, 'floor_covering')
floor_covering = data['floor_covering'].unique()
data['floor_covering_values'] = data['floor_covering'].map({feature: i for i, feature in enumerate(floor_covering)})

"""##HOA"""

preprocessor.convert_to_int(data, 'HOA')

"""#Checking data types after conversions"""

data.dtypes

"""#Check NaN

This line of code computes the number of missing (or NaN) values for each column in a pandas DataFrame called data.

First, the .isna() method is used to create a Boolean DataFrame where True indicates that the value in the original DataFrame is NaN, and False indicates that the value is not NaN.

Then, the .sum() method is called on the Boolean DataFrame, which will count the number of True values (i.e., the number of NaN values) for each column in the original DataFrame. The result is a Series object where each column name is associated with the number of NaN values in that column.
"""

data.isna().sum()

"""## Data types end

#Data description and dupes
"""

data.describe()

data.duplicated().sum()

"""#Column_definitions"""

# Specify the columns to check for outliers
columns_to_check = ['sold_price', 'lot_acres', 'taxes', 'bedrooms', 'bathrooms', 'sqrt_ft', 'garage', 'kitchen_features', 'fireplaces', 'floor_covering', 'HOA', 'kitchen_features_values', 'floor_covering_values']

# Scale or normalize the features
columns_to_scale = ['sold_price', 'lot_acres', 'taxes', 'year_built', 'sqrt_ft', 'fireplaces', 'floor_covering_values', 'bathrooms', 'bedrooms', 'garage']
data_scaled = preprocessor.scale_features(data, columns_to_scale)

"""#Visualise columns checked"""

fig, axs = plt.subplots(ncols=len(columns_to_check), figsize=(35,5))
for i, col in enumerate(columns_to_check):
  axs[i].boxplot(data[col].values)
  axs[i].set_title(col)

plt.show()

"""#Scale data"""

data_scaled = preprocessor.scale_features(data, columns_to_scale)

"""#Check for outliers start"""

data.info()

"""##Correlation Matrix prior to removing outliers"""

data_processor = DataPreprocessor(data)

data_processor.display_correlation_matrix(columns_to_check)

"""##Removing outliers"""

data_no_outliers = preprocessor.remove_outliers(data_scaled, columns_to_check, threshold=3)

"""##Checking outliers after outliers are removed"""

data_processor = DataPreprocessor(data_no_outliers)

data_processor.display_correlation_matrix(columns_to_scale)

"""#Class example"""

from geopy import Nominatim

"""Give it user location"""

geolocator = Nominatim(user_agent = 'mAIstros')

"""variable name we are giving it"""

location = geolocator.geocode("1785 The Exchange SE, Atlanta")

"""Let's look at the location"""

location

"""look at the point"""

location.point

location = geolocator.geocode("10 Downing Street")

location

"""calc location point"""

print("Latitude: {}, Longitude: {}".format(location.point.latitude, location.point.longitude))

"""#Old code that is now object oriented"""

#non_numeric = data[pd.to_numeric(data['bathrooms'], errors='coerce').isna()]
#print(non_numeric)

#Replace "None" with NaN
#data['sqrt_ft'] = data['sqrt_ft'].replace("None", np.nan)

#Convert the columns to float
#data['sqrt_ft'] = data['sqrt_ft'].astype(float)

#Replace NaN with 0
#data['sqrt_ft'] = data['sqrt_ft'].fillna(0)

#Round down to the nearest integer
#data['sqrt_ft'] = data['sqrt_ft'].apply(lambda x: math.floor(x))

#Convert to int
#data['sqrt_ft'] = data['sqrt_ft'].astype(int)

#Replace "None" with NaN
#data['garage'] = data['garage'].replace("None", np.nan)

#Convert the columns to float
#data['garage'] = data['garage'].astype(float)

#Replace NaN with 0
#data['garage'] = data['garage'].fillna(0)

#Round down to the nearest integer
#data['garage'] = data['garage'].apply(lambda x: math.floor(x))

#Convert to int
#data['garage'] = data['garage'].astype(int)

# Replace "None" with NaN
#data['bathrooms'] = data['bathrooms'].replace("None", np.nan)

# Convert the 'bathrooms' column to float
#data['bathrooms'] = data['bathrooms'].astype(float)

# Replace NaN with 0
#data['bathrooms'] = data['bathrooms'].fillna(0)

# Round down to the nearest integer
#data['bathrooms'] = data['bathrooms'].apply(lambda x: math.floor(x))

# Convert to int
#data['bathrooms'] = data['bathrooms'].astype(int)

# Verify the result
#print(data['bathrooms'].unique())

#def kitchen_features_count(features):
#    return len(features.split(','))

#data['kitchen_features'].value_counts()

# Create a new column with the counts
#data['kitchen_features_count'] = data['kitchen_features'].map(kitchen_features_count)

# Calculate the correlation matrix
#correlation_matrix = data.corr()

#fig, ax = plt.subplots(figsize=(10, 8))
#im = ax.imshow(correlation_matrix, cmap='coolwarm')

# Display the colorbar
#cbar = ax.figure.colorbar(im, ax=ax)

# Set the tick labels and axis labels
#ax.set_xticks(np.arange(len(columns_to_check)))
#ax.set_yticks(np.arange(len(columns_to_check)))
#ax.set_xticklabels(columns_to_check, fontsize=12)
#ax.set_yticklabels(columns_to_check, fontsize=12)
#ax.set_xlabel('Features', fontsize=14)
#ax.set_ylabel('Features', fontsize=14)

# Rotate the tick labels and set their alignment
#plt.setp(ax.get_xticklabels(), rotation=45, ha="right",
         #rotation_mode="anchor")

# Display the correlation values in the heatmap
#for i in range(len(columns_to_check)):
    #for j in range(len(columns_to_check)):
      #text = ax.text(j, i, f'{correlation_matrix.iloc[i, j]:.2f}', ha="center", va="center", color="w", fontsize=10)

# Set the title of the plot
#ax.set_title("Correlation Matrix", fontsize=16)

# Show the plot
#plt.show()